## PCA、SVD以及SVD和协方差矩阵的关联

### 1. 什么是PCA

PCA(Principal Component Analysis) 主成分分析法是一种降维方法。顾名思义，就是提取出数据**主要**信息的一种方法。

它会通过筛选出最重要的k个主要成分(Principal Components)，然后将数据映射在这些主要成份上，来获取数据的主要信息。

### 2. 什么是主要成分

在PCA中，PC(Principal Component)是非常重要的数据，它对应于特征值从大到小排列的特征向量。我们将这些特征向量，称作主要成分。

### 3. PCA with covariance

PCA有两种表示形式，第一种是协方差表示形式。

$$
cov(X,Y) = \frac{\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}{n-1}
$$
我们可以对协方差矩阵 $cov(X,Y)$做特征值分解，分解得到特征值和特征向量，选取特征值最大的前k个特征向量即可。

PCA具体流程可以分为四个部分。

* 数据集中化

  在这个部分中，我们要把数据原点移到数据的质心。这么做是为了计算方差的时候方便。
  $$
  \hat{X} = X - m
  $$
  m是均值矩阵（每个数据的平均值，行代表数据点那么就是跨行求平均，即把每行数据加在一起求平均得到(1,n)的矩阵；反之就是得到一个(m,1)的矩阵）。

* 求协方差矩阵
  $$
  cov(X,Y) = \frac{\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}{n-1}
  $$
在线性代数中，
$$
cov(X,Y) = \frac{1}{n-1}A^TA
$$
 	 可以自己证明，不要问为什么，代数字进去算算就明白。

* 特征值分解

  可以使用`python`中的`np.linalg.eig()`求特征值。手动求特征值请参照线性代数课本。举个例子：

  ```python
  import numpy as np
  
  A = np.random.rand(3, 5)
  print(f'A:\n {A}')
  B = np.random.rand(5, 3)
  print(f'B:\n {B}')
  
  A:
   [[2.32097658e-01 8.69667156e-01 4.84704212e-01 5.25999376e-01
    4.91347784e-01]
   [2.68178184e-01 4.94268659e-01 1.06246931e-01 3.53954676e-01
    2.60253588e-01]
   [4.63476477e-01 7.08619753e-01 5.11177454e-01 5.06374713e-04
    3.28189071e-01]]
  B:
   [[0.91514208 0.92206441 0.50696091]
   [0.54801509 0.04371969 0.07101968]
   [0.05481562 0.12848639 0.14812604]
   [0.46152671 0.1618032  0.98097817]
   [0.23866374 0.99569726 0.92332776]]
  
  wA, vA = np.linalg.eig(np.cov(A))
  print(wA)
  print(vA)
  
  [0.00000000e+00 1.99003025e-01 2.86644970e-01 1.45857798e-17
   7.55788750e-19]
  [[ 0.87767663 -0.32649542 -0.35083396 -0.35523744  0.85975886]
   [-0.0876903   0.27672108 -0.47689766  0.79244098 -0.17778532]
   [ 0.02778817 -0.02204063  0.09002889 -0.01540209 -0.1463089 ]
   [ 0.45687615  0.7892349   0.40847798 -0.16475853  0.45097583]
   [ 0.11174137 -0.439828    0.68885753  0.46739818  0.06649494]]
  ```

  假设我们选择前2个特征向量作为PC1和PC2（假设行为特征，列为数据点），那么PC1 = [ 0.87767663 -0.32649542 -0.35083396 -0.35523744  0.85975886], PC2 = [-0.0876903   0.27672108 -0.47689766  0.79244098 -0.17778532]

* 空间映射

  最后将原数据映射在高维空间上
  $$
  z = p^T\hat{X}
  $$
  P代表PC1,PC2组合而来的矩阵。

### 4. PCA with SVD

SVD算法可以通过不求方差得到特征向量。
$$
A = U\sigma V^T
$$
在这个分解中，有
$$
UU^T=I,VV^T=I
$$
联想到
$$
cov(X,Y) = \frac{1}{n-1}A^TA
$$

我们可以求得
$$
A^TA = V\sigma ^TU^T U\sigma V^T = V\sigma ^T\sigma V^T
$$
如果我们在求SVD时令
$$
\hat{A} = \frac{1}{\sqrt{n-1}}A
$$
那么V就是特征向量， $\sigma^2$ 就是特征值。

使用SVD的PCA算法可以把第二三步合并，直接使用`np.linalg.svd()`求解即可。

### 5. 为什么PCA能降维？

因为从线性代数的几何角度来讲，一个向量乘常数，代表大小变化，乘向量，代表方向转变。因此做特征值（或SVD）分解的时候，我们其实就是在找向量和特征值的组合来表达原来的数据。特征值大，就代表在那个方向上的向量的信息大。故PCA可以降维。

此外，SVD分解方法和方差方法得出的结果一致。原因可以参照4。