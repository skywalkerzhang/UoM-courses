{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Coursework for k-means, spectral agglomerative clustering algorithms,\n",
    "as well as ensemble clustering.\n",
    "\"\"\" \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import cluster,datasets\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. K-means Clustering Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== Assignment 1 ===========</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### K-means Clustering Analysis #######################################\n",
    "# Assignment 1:\n",
    "# ===\n",
    "\n",
    "X = np.load('./Data/kmeans_data_1.npy')\n",
    "\n",
    "mu_a = np.array([[5,-10],[-5,5],[-15,-10]]) # The given initial mean points of (a) \n",
    "mu_b = np.array([[-5,0],[-5,0.2],[-5.2,0]]) # The given initial mean points of (b) \n",
    "mu_c = np.array([[-6,-5],[1.9,1.9],[2.,2.]]) # The given initial mean points of (c) \n",
    "\n",
    "def partition(X, mu):\n",
    "    \"\"\"\n",
    "    Helper function that performs a partition given the mean points using Euclidean distance\n",
    "    Each input is numpy array:\n",
    "        - X: (N,d) or (N,C,d), data points\n",
    "        - mu: (C x d), the mean points\n",
    "    \n",
    "    Returns:\n",
    "        partition: (N), the label of partition that each datapoint belongs to\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = mu.shape[0] # number of clusters\n",
    "    ###\n",
    "    #   You code here\n",
    "    ###\n",
    "    #tile x (N,d) to (N,C,D)\n",
    "    if len(np.shape(X)) == 2:\n",
    "        x_CND = np.tile(X,[C,1,1])#(N,d) -> (C,N,d)\n",
    "        x_NCD = np.transpose(x_CND, axes = [1,0,2])\n",
    "    distance_NC = np.sum((x_NCD - mu)**2,axis = 2)\n",
    "    argmin_distance_N = np.argmin(distance_NC,axis = 1)\n",
    "    distance_N = np.sqrt(np.min(distance_NC,axis = 1))\n",
    "    partition = argmin_distance_N\n",
    "    ###\n",
    "    #   End of Your code\n",
    "    ###\n",
    "    return partition\n",
    "\n",
    "############################\n",
    "# a) Implement a display function, where different clusters must be marked in different colours\n",
    "\n",
    "def visualize_kmeans_res(mu,X,lb,title = ''):\n",
    "    #####\n",
    "    #   Your code here\n",
    "    # ===\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###\n",
    "    #   End of your code\n",
    "    ###\n",
    "\n",
    "    \n",
    "#############################\n",
    "# b) Display the final partitions where the initial and final mean points\n",
    "#\n",
    "#   Your code here\n",
    "# ===\n",
    "\n",
    "# Use mu_a as the initial mean point \n",
    "print(\"Use mu_a as the initial mean point: \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use mu_b as the initial mean point \n",
    "print(\"Use mu_b as the initial mean point: \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use mu_c as the initial mean point \n",
    "print(\"Use mu_c as the initial mean point: \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== End of Assignment 1 ===========</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== Assignment 2 ===========</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Assignment 2 \n",
    "#\n",
    "# K-means algorithm cannot be used until the hyperparameter K (the\n",
    "# number of clusters) is set up so the\n",
    "# clustering result is better. We use f_ratio from the lecture\n",
    "# to do so.\n",
    "# ===\n",
    "\n",
    "\n",
    "################\n",
    "# (a) Implement the scatter-based F-ratio index in Python \n",
    "#   where Euclidean distance is used\n",
    "\n",
    "def f_ratio_euclidean(X,lb):\n",
    "    \"\"\"\n",
    "    # Compute the f-ratio = k * ssw / ssb\n",
    "    \n",
    "    Input:\n",
    "        - X: (n,d), n datapoints each with d dimension\n",
    "        - lb: (n,) label of each datapoint, each element is an\n",
    "              integer, >=0, <n.\n",
    "    \n",
    "    Return:\n",
    "        - f_ratio = k * ssw / ssb: scalar\n",
    "    \"\"\" \n",
    "    k = len(np.unique(lb))\n",
    "    _,d = np.shape(X)\n",
    "    n = np.zeros(k)\n",
    "    c = np.zeros([k,d])\n",
    "    ###\n",
    "    # Your code here\n",
    "    ###\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # End of your code\n",
    "    ###\n",
    "    return f_ratio\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(0)\n",
    "################\n",
    "# (b) For K = 2, 3, · · · , 10, run the the K-means each with 3 different random initialisation\n",
    "#  on the 'kmeans_data_2.npy' dataset and plot F-ratio index versus K\n",
    "#\n",
    "# Your code here:\n",
    "# ===\n",
    "X = np.load('./Data/kmeans_data_2.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('default')\n",
    "################\n",
    "# (c) Display the final partition with optimal number of clusters\n",
    "#\n",
    "# Your code here:\n",
    "# ===\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== End of Assignment 2 ===========</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Spectral Clustering Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== Assignment 3 ===========</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Spectral Clustering Analysis ######################################\n",
    "# Assignment 3:\n",
    "#\n",
    "# Here you are to implement the asymmetric normalised spectral clustering algorithm.\n",
    "#\n",
    "# Hint: To implement the asymmetric_SC function, you can\n",
    "#       use the built-in function, np.linalg.eig, in the numpy library for eigen analysis,\n",
    "#       and, pairwise_distances, in sklearn.metrics for compute parewise distance, and the\n",
    "#       sklearn built-in function, cluster.KMeans.\n",
    "# ===\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def gaussian_similarity(X,delta):\n",
    "    \"\"\"\n",
    "    # Compute the similarity matrix for spectral clustering using RBF kernel\n",
    "    # Construction of similarity matrix is diverse and non-trivial, this\n",
    "    # is only one of the types.\n",
    "    # Also, the parameter delta has large influence on the final result\n",
    "\n",
    "    \n",
    "    Input:\n",
    "        - X: (n,d), n datapoints each with d dimension\n",
    "        - delta: scalar, width of RBF kernel\n",
    "    \n",
    "    Return:\n",
    "        - W: (n,n) similarity matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    ###\n",
    "    # Hint: The RBF kernel definition can be found in the lecture notes.\n",
    "\n",
    "\n",
    "def asymmetric_SC(W,k = 1):\n",
    "    \"\"\"\n",
    "    # Compute the spectral clustering according to [Shi and Malik 2000]\n",
    "    # See http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323\n",
    "    Input:\n",
    "        - W: (n,n), similarity matrix or weighted adjacency matrix\n",
    "        - k: scalar, number of clusters\n",
    "            \n",
    "    \n",
    "    Return:\n",
    "        - lb: (n,) label for each datapoint\n",
    "        - e: 1-d array of eigen value\n",
    "        - v: 2-d matrix, each column is an eigen vector\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Hint: Be wary that \"Asymmetric Normalized\" spectral clustering does eigen analysis on D^-1 * L .\n",
    "    #       See lecture slides if you don't know how to start \n",
    "    #\n",
    "    # Hint: Computing the invert matrix is computationally expensive for high-dimensional\n",
    "    #       matrices. But here, we only need to the invert a diagnal matrix, which is simple to compute.\n",
    "    # PS: There is no unreal eigen value here, but sometimes numerical precision problems happen\n",
    "    #\n",
    "    # First do eigen analysis. \n",
    "    #\n",
    "    # Your code here :\n",
    "    ###\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # End of your code\n",
    "    ###\n",
    "    return lb,e,v\n",
    "\n",
    "\n",
    "####### \n",
    "# Now test your implementation with toy data and provided hyper parameters.\n",
    "# You should see a graph showing a clear block matrix, and a reasonable clustering result\n",
    "# PS: this is just for testing. No suggestion of using what parameter\n",
    "# ==\n",
    "X = np.array([\n",
    "    [1, 3], [2, 1], [1, 1],\n",
    "    [3, 2], [7, 8], [9, 8],\n",
    "    [9, 9], [8, 7], [13, 14],\n",
    "    [14, 14], [15, 16], [14, 15]\n",
    "])\n",
    "\n",
    "\n",
    "W = gaussian_similarity(X,delta = pairwise_distances(X).std())\n",
    "plt.figure()\n",
    "plt.title('gaussian similarity matrix (white = high,black = low)')\n",
    "plt.imshow(W,cmap = 'gray')\n",
    "\n",
    "lb,_,_ = asymmetric_SC(W,3)\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0],X[:,1],c = lb)\n",
    "plt.title('datapoint and clustering result')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== End of Assignment 3 ===========</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== Assignment 4 ===========</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### \n",
    "# Assignment 4:\n",
    "#\n",
    "# Here you are to apply your implemented gaussian_similarity function to a dataset,\n",
    "# saved in './Data/SC_data_1.npy'. You need to do:\n",
    "#   a) find out an appropriate hyperparameter value in the Gaussian kernel\n",
    "#   b) calculate and report the first non-zero eigen vector of laplacian L\n",
    "#   c) decide how many clusters are in this data set and display results (you will work on this in the next cell)\n",
    "#\n",
    "# PS: in b), please do eigen analysis of 'L', not D^-1 L.\n",
    "# ===\n",
    "X = np.load('./Data/SC_data_1.npy')\n",
    "###\n",
    "# a) & b), You code here:\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "# End of your code\n",
    "#\n",
    "# Fill your result below:\n",
    "###\n",
    "print('a) One hyperparameter value for Gaussian kernel:', ?)\n",
    "print('b) First non-zero eigen vector:', ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Decide the number of clusters in this data set and display results \n",
    "# Your code here:\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "#  End of your code\n",
    "#\n",
    "#  Fill your answer below:\n",
    "###\n",
    "print('c) ', ?, ' clusters, because ??? ' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== End of Assignment 4 ===========</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== Assignment 5 ===========</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### \n",
    "# Assignment 5:\n",
    "#\n",
    "# Here you are to apply your implemented gaussian_similarity function to \n",
    "# more datasets and display results. You need to find proper hyper parameters\n",
    "# ===\n",
    "X1 = np.load('./Data/SC_data_1.npy')\n",
    "X2 = np.load('./Data/SC_data_2.npy')\n",
    "X3 = np.load('./Data/SC_data_3.npy')\n",
    "###\n",
    "#  Your code here\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== End of Assignment 5 ===========</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Hierarchical Clustering Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== Assignment 6 ===========</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### \n",
    "# Assignment 6:\n",
    "#\n",
    "# Here, you are to use built-in functions in scipy.cluster.hierarchy: 'linkage, dendrogram' to \n",
    "# do hierachical clustering.\n",
    "#\n",
    "# You need to: a) plot three dendrogram trees achieved by the use of\n",
    "# three cluster-distance measures in the agglomerative algorithm; (b) report the number of clusters\n",
    "# found by using the longest K-cluster lifetime criterion achieved from (a), respectively; \n",
    "# (c) display 3 clustering results achieved by (a) and (b)\n",
    "# ===\n",
    "from scipy.cluster.hierarchy import fcluster, linkage, dendrogram\n",
    "\n",
    "X = np.load('./Data/HC_data.npy')\n",
    "\n",
    "\n",
    "# a):\n",
    "# A (?,4) shape matrix Z should returned. At the i-th iteration, \n",
    "#   clusters with indices Z[i, 0] and Z[i, 1] are combined \n",
    "#   to form cluster . A cluster with an index less than n\n",
    "#   corresponds to one of the n original observations. \n",
    "#   The distance between clusters Z[i, 0] and Z[i, 1] is \n",
    "#   given by Z[i, 2]. The fourth value Z[i, 3] represents \n",
    "#   the number of original observations in the newly formed cluster.\n",
    "#\n",
    "#   Then, plot the clustering result as a Dendrogram\n",
    "#       The dendrogram illustrates how each cluster is composed by drawing a U-shaped\n",
    "#       (or n-shaped) link between a non-singleton cluster and its children. The top of the \n",
    "#       U-link indicates a cluster merge. The two legs of the U-link indicate \n",
    "#       which clusters were merged. The horizental coordinates of a merge \n",
    "#       represents the distance between the two clusters. \n",
    "#\n",
    "#  Your code here\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (b) Now, report the number of clusters\n",
    "# found by using the longest K-cluster lifetime criterion achieved from (a)\n",
    "#\n",
    "# Fill your answer here:\n",
    "print('b)                              ')\n",
    "\n",
    "# (c) Display clustering results\n",
    "# Your code here:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== End of Assignment 6 ===========</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Ensemble Clustering Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== Assignment 7 ===========</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### \n",
    "# Assignment 7:\n",
    "#\n",
    "# Here, you are to implement the evidence-accumulated clustering algorithm based on K-means \n",
    "# and the agglomerative algorithms in Python.\n",
    "#\n",
    "# Hint: To implement the ensemble_clustering function, you can use the\n",
    "# build-in functions, sklearn.cluster.KMeans , in the scikit-learn library for K-means cluster-\n",
    "# ing, the scipy.cluster.hierarchy.linkage and scipy.spatial.distance.squareform in the\n",
    "# scipy library for hierarchical clustering\n",
    "# ===\n",
    "\n",
    "\n",
    "# ============\n",
    "# Compute co-association matrix\n",
    "# ============\n",
    "# Let's firstly compute the co-association matrix for ONLY SINGLE cluster result\n",
    "\n",
    "def compute_co_matrix_for_single_cluster(y):\n",
    "    \"\"\"\n",
    "    # Compute the co-association matrix for single clustering result\n",
    "    # It will also be used later to compute multiple clustering result\n",
    "    \n",
    "    Input:\n",
    "        - y: (n) Label of each Datapoint\n",
    "    \n",
    "    Return:\n",
    "        - co_matrix: (n,n). the (i,j) element is the number of same cluster components\n",
    "                      that datapoint X[i] and datapoint X[j] belong to. Surely,\n",
    "                      in the case of single cluster result, this number will be\n",
    "                      either 0 or 1. Whether they belong to the same cluster component\n",
    "                      can be judged by 'y[i] == y[j]'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the co-association matrix\n",
    "    n = y.shape[0]\n",
    "    co_matrix = np.zeros(shape=(n,n))\n",
    "\n",
    "    ###    \n",
    "    # Your code here\n",
    "    #\n",
    "    # To do: finish the computation of co_matrix for single cluster\n",
    "    ###\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # End of your code\n",
    "    ###                \n",
    "    return co_matrix\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "y = np.array([0,1,0,1,0])\n",
    "co_matrix_tmp = compute_co_matrix_for_single_cluster(y)\n",
    "co_matrix_tmp\n",
    "# It should show a square symmetric matrix, where each element is 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compute the co-association matrix for MULTIPLE cluster results\n",
    "def compute_co_matrix(y_list):\n",
    "    \"\"\"\n",
    "    # Compute the co-association matrix\n",
    "    \n",
    "    Input:\n",
    "        - y_list: List of lenth (N). Each element is a clustering label vector,\n",
    "                  which is the same as the 'y' defined in function \n",
    "                  'compute_co_matrix_for_single_cluster(y)'\n",
    "                  N is the number of clustering results you are to ensemble.\n",
    "    \n",
    "    Return:\n",
    "        - co_matrix: (N,N). the (i,j) element is the number of same cluster components\n",
    "                      that datapoint X[i] and datapoint X[j] belong to, divided\n",
    "                      by N. \n",
    "                      \n",
    "                      \n",
    "    \"\"\"\n",
    "    N = len(y_list)\n",
    "    \n",
    "    # Initialize matrix\n",
    "    n = y_list[0].shape[0]\n",
    "    co_matrix = np.zeros([n,n],dtype = np.float32)\n",
    "\n",
    "    ###   \n",
    "    # Your code here    \n",
    "    #\n",
    "    # Hint: Use the 'compute_co_matrix_for_single_cluster(y)' you have finished\n",
    "    #       to compute the co-association matrix for each cluster result,\n",
    "    #       then add them together. Don't forget to normalize.\n",
    "    #\n",
    "    ###\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # End of your code\n",
    "    ###\n",
    "    return co_matrix\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "y1 = np.array([0,1,0,1,0])\n",
    "y2 = np.array([0,1,1,1,0])\n",
    "y3 = np.array([0,1,1,1,1])\n",
    "co_matrix_tmp = compute_co_matrix([y1,y2])\n",
    "co_matrix_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============\n",
    "# Implement the ensemble clustering given a co_matrix\n",
    "# ============\n",
    "import scipy\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "def ensemble_clustering(co_matrix, link_type = 'single'):\n",
    "    \"\"\"\n",
    "    Ensamble the clustering results with Agglomerative Clustering\n",
    "    \n",
    "    Input: \n",
    "        - co_matrix: (n,n) co-assocication matrix. n is the number of datapoints. \n",
    "        - linkage: string, can be one of {'single','ward','average','complete'}\n",
    "        \n",
    "    Output: \n",
    "        - Z: a (?,4) matrix, specifying how datapoints are clustered. \n",
    "             At the i-th iteration, clusters with indices Z[i, 0]\n",
    "             and Z[i, 1] are combined to form cluster. A cluster \n",
    "             with an index less than corresponds to one of the \n",
    "             original observations. The distance between clusters Z[i, 0] \n",
    "             and Z[i, 1] is given by Z[i, 2]. The fourth value Z[i, 3] \n",
    "             represents the number of original observations in the newly \n",
    "             formed cluster.\n",
    "             \n",
    "             Z should be the return value of 'scipy.cluster.hierarchy.linkage'\n",
    "             function.\n",
    "    \"\"\"\n",
    "    ###\n",
    "    # Your code here\n",
    "    #\n",
    "    # To do: ensemble the clustering results with Agglomerative Clustering\n",
    "    # Hint: see 'scipy.cluster.hierarchy.linkage' and \n",
    "    #           'scipy.spatial.distance.squareform'\n",
    "    #\n",
    "    # Hint2: 'squareform' function requires zero element on diagonal\n",
    "    ###\n",
    "\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # End of your code\n",
    "    ###\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== End of Assignment 7 ===========</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== Assignment 8 ===========</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### \n",
    "# Assignment 8:\n",
    "#\n",
    "# Here, you are to Apply your implemented ensemble_clustering function to two datasets,\n",
    "# SC_data_2.npy and SC_data_3.npy\n",
    "#\n",
    "# You will need to first get a list of clustering results by K-means (you also \n",
    "# need to choose the n_clusters), you can use the built in K-means function in sklearn.cluster.\n",
    "#\n",
    "# Then you will need to compute the co_matrix with your implemented function, and then\n",
    "# get and display the clustering result with your implementation. Meanwhile, you need \n",
    "# to choose a proper distance measurement (the 'linkage' parameter) to make your algorithm\n",
    "# work.\n",
    "#\n",
    "# ===\n",
    "np.random.seed(1) # Ensure the results are same on submission\n",
    "X = np.load('./Data/SC_data_2.npy')\n",
    "###\n",
    "# Now apply your ensemble clustering to the first dataset './Data/SC_data_2.npy', display results\n",
    "#\n",
    "# Your code here \n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('./Data/SC_data_3.npy')\n",
    "\n",
    "np.random.seed(1)\n",
    "# Now apply your ensemble clustering to the second dataset, './Data/SC_data_3.npy', display results\n",
    "#\n",
    "# Your code here \n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">=========== End of Assignment 8 ===========</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
